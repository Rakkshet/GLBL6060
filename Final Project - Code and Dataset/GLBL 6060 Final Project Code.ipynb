{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49ccc03b-f4ff-4e90-bc57-07031ce99c77",
   "metadata": {},
   "source": [
    "# GLBL 6060: Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a370d1-3f68-4d1f-9e0f-ccb1565ee576",
   "metadata": {},
   "source": [
    "## Research Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e548d8-ab6d-484b-9516-de05a56269da",
   "metadata": {},
   "source": [
    "How have the patterns of political contributions from the agricultural sector in India shifted since 2010, specifically towards the two major national political parties (Indian National Congress and Bharatiya Janata Party)? What trends might emerge in the future regarding the nature and extent of these donations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8487924-2206-477b-bd1e-6143b5cf5b40",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e16be82-a661-4cae-908b-cb39729dbbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy pandas scikit-learn tensorflow matplotlib requests beautifulsoup4 tqdm folium geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d23bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fundamental package for scientific computing with Python\n",
    "import numpy as np\n",
    "\n",
    "# Data analysis and manipulation tool\n",
    "import pandas as pd\n",
    "\n",
    "# Library for creating static, animated, and interactive visualizations in Python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Library to send HTTP requests for pulling data from web servers\n",
    "import requests\n",
    "\n",
    "# Module to generate a new feature matrix consisting of all polynomial combinations of the features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Utilities for cross-validation and hyperparameter tuning\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "\n",
    "# Convert a collection of text documents to a matrix of token counts\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Compute cosine similarity between samples in two sets of data\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# TensorFlow and Keras are used for building and training neural network models\n",
    "from tensorflow.keras.models import Sequential  # Base model class for sequential layers\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization  # Different types of neural network layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping  # Callback to stop training when a monitored metric has stopped improving\n",
    "\n",
    "# Scikit-learn-style wrapper for Keras models\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "# Library for parsing HTML and XML documents\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Concurrent execution module to run code in parallel, used for web scraping\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# A library for showing progress bars in loops during execution\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Library for encoding URL parameters\n",
    "from urllib.parse import quote\n",
    "\n",
    "# A package that simplifies working with geographic data\n",
    "import geopandas as gpd\n",
    "\n",
    "# A package creates interactive maps\n",
    "import folium\n",
    "\n",
    "# Library that includes advance features such as heatmaps, minimaps, timestamped layers, and more\n",
    "import folium.plugins as plugins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6989296a-04bf-4e6d-9177-a01a7f783f3a",
   "metadata": {},
   "source": [
    "## Data Acquisition - Web Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a2a787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_donations(base_url, total_pages):\n",
    "    \"\"\"\n",
    "    Fetches all donations from multiple pages of a website and returns them as a list of dictionaries.\n",
    "\n",
    "    Parameters:\n",
    "        base_url (str): The base URL of the website to fetch donations from.\n",
    "        total_pages (int): The total number of pages containing donation information.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing donation information. Each dictionary represents a donation and contains the following keys:\n",
    "            - 'donor' (str): The name of the donor.\n",
    "            - 'address' (str): The address of the donor. If the address is not available, it is an empty string.\n",
    "            - 'amount' (str): The amount of donation.\n",
    "            - 'year' (str): The year in which the donation was made.\n",
    "    \"\"\"\n",
    "    all_donations = []  # List to store all donations\n",
    "\n",
    "    # Loop through each page to fetch donations\n",
    "    for page in tqdm(range(1, total_pages + 1), desc=\"Fetching pages\"):\n",
    "        url = f\"{base_url}&page={page}\"  # Construct URL for current page\n",
    "        \n",
    "        # Send request to fetch HTML content of the page\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')  # Parse HTML using BeautifulSoup\n",
    "\n",
    "        # Extract rows containing donation information\n",
    "        rows = soup.find_all('tr')\n",
    "        \n",
    "        # Iterate through each row and extract donation details\n",
    "        for row in rows[1:]:\n",
    "            cols = row.find_all('td')\n",
    "            \n",
    "            # Check if there are enough columns to extract donation details\n",
    "            if len(cols) >= 7:\n",
    "                all_donations.append({\n",
    "                    'donor': cols[1].text.strip(),\n",
    "                    'address': cols[2].text.strip() if cols[2].text.strip() != 'Not Available' else '',\n",
    "                    'amount': cols[3].text.strip(),\n",
    "                    'year': cols[6].text.strip(),\n",
    "                })\n",
    "    return all_donations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a4da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The base URL for fetching donation information for BJP\n",
    "base_url = \"https://myneta.info/party/index.php?action=all_donors&id=3\"\n",
    "total_pages = 194\n",
    "\n",
    "# Fetch donations from the specified URL and total pages\n",
    "donations = fetch_all_donations(base_url, total_pages)\n",
    "\n",
    "# Write donations to a CSV file\n",
    "csv_file = \"BJP_donations.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=donations[0].keys())\n",
    "    writer.writeheader()  # Write header row with column names\n",
    "    for donation in donations:\n",
    "        writer.writerow(donation)  # Write each donation as a row\n",
    "\n",
    "# Print confirmation message\n",
    "print(f\"Data written to {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1ff299-30c4-47f1-a641-cbeaa495f867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The base URL for fetching donation information for INC\n",
    "base_url = \"https://myneta.info/party/index.php?action=all_donors&id=1\"\n",
    "total_pages = 54\n",
    "\n",
    "# Fetch donations from the specified URL and total pages\n",
    "donations = fetch_all_donations(base_url, total_pages)\n",
    "\n",
    "# Write donations to a CSV file\n",
    "csv_file = \"INC_donations.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=donations[0].keys())\n",
    "    writer.writeheader()  # Write header row with column names\n",
    "    for donation in donations:\n",
    "        writer.writerow(donation)  # Write each donation as a row\n",
    "\n",
    "# Print confirmation message\n",
    "print(f\"Data written to {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefc0fad-89ae-41a2-9b9e-45eaa5ea52d4",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aefde43-4612-43c9-bc20-6c4167fa63a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read BJP data\n",
    "df_bjp = pd.read_csv('/Users/rakkshetsinghaal/Desktop/Yale University/GLBL 6060/GLBL6060/Final Project - Code and Dataset/BJP_data.csv')\n",
    "\n",
    "# Read INC data\n",
    "df_inc = pd.read_csv('/Users/rakkshetsinghaal/Desktop/Yale University/GLBL 6060/GLBL6060/Final Project - Code and Dataset/INC_data.csv')\n",
    "\n",
    "# Add 'party' column with respective party names\n",
    "df_bjp['party'] = 'BJP'\n",
    "df_inc['party'] = 'INC'\n",
    "\n",
    "# Filter observations within the year 2010-2022\n",
    "df_bjp = df_bjp[(df_bjp['Year'] >= 2010) & (df_bjp['Year'] <= 2022)]\n",
    "df_inc = df_inc[(df_inc['Year'] >= 2010) & (df_inc['Year'] <= 2022)]\n",
    "\n",
    "# Concatenate DataFrames\n",
    "df_combined = pd.concat([df_bjp, df_inc], ignore_index=True)\n",
    "\n",
    "# Split 'amount' column into separate columns based on space for combined DataFrame\n",
    "split_columns = df_combined['amount'].str.split(' ', expand=True)\n",
    "\n",
    "# Concatenate split columns with the original DataFrame\n",
    "df_combined = pd.concat([df_combined, split_columns], axis=1)\n",
    "\n",
    "# Columns to drop from the DataFrame\n",
    "columns_to_drop = ['amount', 0, 2]\n",
    "\n",
    "# Drop specified columns from the DataFrame\n",
    "df_combined = df_combined.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "# Rename column '1' to 'amount'\n",
    "df_combined.rename(columns={1: 'amount'}, inplace=True)\n",
    "\n",
    "# Drop rows with NaN values in 'amount' column\n",
    "df_combined.dropna(subset=['amount'], inplace=True)\n",
    "\n",
    "# Write combined DataFrame to a new CSV file\n",
    "df_combined.to_csv('/Users/rakkshetsinghaal/Desktop/Yale University/GLBL 6060/GLBL6060/Final Project - Code and Dataset/combined_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb96faa-d627-4970-8d3e-d0ff9ef8bbe0",
   "metadata": {},
   "source": [
    "## Identification of Donations from Agriculutre Sector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee45aa7-7c77-48c8-9556-0f03c299730c",
   "metadata": {},
   "source": [
    "### Web Scrapping Donor Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769f37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    \"\"\"\n",
    "    Load data from a CSV file into a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        filepath (str): The file path of the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The loaded DataFrame containing the data.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(filepath)\n",
    "\n",
    "def search_donor_info(donor_name, api_key):\n",
    "    \"\"\"\n",
    "    Search for information about a donor using the ScrapingBee API.\n",
    "\n",
    "    Parameters:\n",
    "        donor_name (str): The name of the donor to search for.\n",
    "        api_key (str): The API key for accessing the ScrapingBee API.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing information about the donor.\n",
    "    \"\"\"\n",
    "    encoded_name = quote(f'\"{donor_name}\"')\n",
    "    google_url = f\"https://www.google.com/search?q={encoded_name}&num=5\"\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            url=\"https://app.scrapingbee.com/api/v1/\",\n",
    "            params={\n",
    "                \"api_key\": api_key,\n",
    "                \"url\": google_url,\n",
    "                \"custom_google\": \"true\"\n",
    "            },\n",
    "            timeout=10\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return [response.text[i:i + 20000] for i in range(0, len(response.text), 20000)]\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        return [f\"HTTP Error: {str(errh)}\"]\n",
    "    except requests.exceptions.ConnectionError as errc:\n",
    "        return [f\"Connection Error: {str(errc)}\"]\n",
    "    except requests.exceptions.Timeout as errt:\n",
    "        return [f\"Timeout Error: {str(errt)}\"]\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        return [f\"Error: {str(err)}\"]\n",
    "\n",
    "def vector_analysis(df):\n",
    "    \"\"\"\n",
    "    Perform vector analysis on donor information to identify agriculture-related content.\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): The DataFrame containing donor information.\n",
    "\n",
    "    Returns:\n",
    "        pandas.Series: Series containing similarity scores of each donor's information to agriculture-related keywords.\n",
    "    \"\"\"\n",
    "    text = [' '.join([str(row[col]) for col in df.columns if col.startswith('donor_info')]) for _, row in df.iterrows()]\n",
    "    keywords = [\n",
    "        'agriculture', 'farm', 'fertilizer', 'chemical', 'pesticide', 'herbicide', 'crop', 'irrigation', 'harvest',\n",
    "        'tractor', 'agronomy', 'horticulture', 'harvester', 'agricultural machinery', 'farm tools', 'organic farming',\n",
    "        'sustainable agriculture', 'soil health', 'soil nutrition', 'agri-tech', 'livestock', 'poultry', 'manure',\n",
    "        'agribusiness', 'agrochemical', 'farm management', 'crop rotation', 'crop protection'\n",
    "    ]\n",
    "    vectorizer = CountVectorizer(vocabulary=keywords)\n",
    "    X = vectorizer.fit_transform(text)\n",
    "    similarity_scores = cosine_similarity(X, vectorizer.transform([' '.join(keywords)]).toarray())\n",
    "    return pd.Series(similarity_scores.ravel(), index=df.index)\n",
    "\n",
    "def process_donors_concurrently(df, api_key, savepath):\n",
    "    \"\"\"\n",
    "    Use concurrent execution to process donors in batches and delete 'donor_info' cells after vector analysis.\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): The DataFrame containing donor information.\n",
    "        api_key (str): The API key for accessing the ScrapingBee API.\n",
    "        savepath (str): The file path to save the processed data.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    batch_size = 100\n",
    "    batch_data = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor, open(savepath, 'a') as f:\n",
    "        future_to_donor = {executor.submit(search_donor_info, donor, api_key): donor for donor in df['donor']}\n",
    "        for future in tqdm(concurrent.futures.as_completed(future_to_donor), total=len(df['donor']), desc=\"Processing Donors\"):\n",
    "            donor_name = future_to_donor[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                donor_data = {'donor': donor_name}\n",
    "                for i, info in enumerate(result):\n",
    "                    donor_data[f'donor_info {i+1}'] = info\n",
    "                temp_df = pd.DataFrame([donor_data])\n",
    "                temp_df['agricontentcheck'] = vector_analysis(temp_df)\n",
    "                # Drop 'donor_info' columns\n",
    "                temp_df.drop([col for col in temp_df.columns if col.startswith('donor_info')], axis=1, inplace=True)\n",
    "                batch_data.append(temp_df)\n",
    "                \n",
    "                if len(batch_data) >= batch_size:\n",
    "                    batch_df = pd.concat(batch_data)\n",
    "                    batch_df.to_csv(f, header=f.tell()==0, index=False)\n",
    "                    batch_data = []  # Reset the batch data list\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing donor {donor_name}: {e}\")\n",
    "\n",
    "        if batch_data:  # Save any remaining data that didn't fill the last batch\n",
    "            batch_df = pd.concat(batch_data)\n",
    "            batch_df.to_csv(f, header=f.tell()==0, index=False)\n",
    "\n",
    "def main(filepath, savepath, api_key):\n",
    "    \"\"\"\n",
    "    Main function to initiate the processing of donor data and save results incrementally.\n",
    "\n",
    "    Parameters:\n",
    "        filepath (str): The file path of the CSV file containing donor data.\n",
    "        savepath (str): The file path to save the processed data.\n",
    "        api_key (str): The API key for accessing the ScrapingBee API.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    df = load_data(filepath)\n",
    "    process_donors_concurrently(df, api_key, savepath)\n",
    "    print(\"Processing completed and data saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main('/Users/rakkshetsinghaal/Desktop/Yale University/GLBL 6060/GLBL6060/Final Project - Code and Dataset/combined_data.csv', '/Users/rakkshetsinghaal/Desktop/Yale University/GLBL 6060/GLBL6060/Final Project - Code and Dataset/donor_combined_data.csv', 'AG94RNW5SQWWUMSNEOISLUP7NSIU5RLOVNHMU6QAGABOSNAUFJP2HSVGPH36DB9NVBGT8VHBP98EI7WA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8745643-846c-4add-8c2b-5fd1f6853d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df = pd.read_csv('/Users/rakkshetsinghaal/Desktop/Yale University/GLBL 6060/GLBL6060/Final Project - Code and Dataset/donor_combined_data.csv')\n",
    "\n",
    "# Create a new column 'agri_content' based on the condition\n",
    "df['agri_content'] = df['agricontentcheck'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Write the modified DataFrame back to the CSV file\n",
    "df.to_csv('/Users/rakkshetsinghaal/Desktop/Yale University/GLBL 6060/GLBL6060/Final Project - Code and Dataset/updated_donor_combined_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78227ea7-e896-4ddb-b5e6-54413cdae359",
   "metadata": {},
   "source": [
    "### Geocoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a655cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates(address, api_key):\n",
    "    \"\"\"\n",
    "    Get latitude and longitude coordinates for a given address using the Geoapify Geocoding API.\n",
    "\n",
    "    Parameters:\n",
    "        address (str): The address to geocode.\n",
    "        api_key (str): The API key for accessing the Geoapify Geocoding API.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing latitude and longitude coordinates (latitude, longitude).\n",
    "               Returns (None, None) if coordinates are not found.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.geoapify.com/v1/geocode/search?text={address}&limit=1&apiKey={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if data[\"features\"]:\n",
    "            result = data[\"features\"][0]\n",
    "            latitude = result[\"geometry\"][\"coordinates\"][1]\n",
    "            longitude = result[\"geometry\"][\"coordinates\"][0]\n",
    "            return latitude, longitude\n",
    "        else:\n",
    "            return None, None\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def process_addresses(filepath, savepath, api_key):\n",
    "    \"\"\"\n",
    "    Process addresses in a CSV file by geocoding them and save the updated CSV.\n",
    "\n",
    "    Parameters:\n",
    "        filepath (str): The file path to the input CSV file.\n",
    "        savepath (str): The file path to save the modified CSV file.\n",
    "        api_key (str): The API key for accessing the Geoapify Geocoding API.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)  # Read CSV file into DataFrame\n",
    "    df['latitude'] = None  # Initialize Latitude column with None values\n",
    "    df['longitude'] = None  # Initialize Longitude column with None values\n",
    "\n",
    "    # Iterate over rows and geocode addresses\n",
    "    batch_size = 100\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Geocoding in Process\"):\n",
    "        address = row['address']\n",
    "        latitude, longitude = get_coordinates(address, api_key)\n",
    "        df.at[index, 'latitude'] = latitude  # Assign latitude to corresponding row\n",
    "        df.at[index, 'longitude'] = longitude  # Assign longitude to corresponding row\n",
    "\n",
    "        # Save after every batch_size rows\n",
    "        if index % batch_size == 0 and index != 0:\n",
    "            df.to_csv(savepath, index=False)\n",
    "            print(f\"Partial CSV saved at {savepath}\")\n",
    "\n",
    "    # Save the final DataFrame\n",
    "    df.to_csv(savepath, index=False)  # Save updated DataFrame to CSV\n",
    "    print(\"Final CSV saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the process_addresses function with input and output file paths and API key\n",
    "    process_addresses('/Users/rakkshetsinghaal/Desktop/Yale University/GLBL 6060/GLBL6060/Final Project - Code and Dataset/updated_donor_combined_data.csv', '/Users/rakkshetsinghaal/Desktop/Yale University/GLBL 6060/GLBL6060/Final Project - Code and Dataset/address_donor_combined_data.csv', 'a4229f468dd4409d95c6995b770fd835')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd36e86-5e23-43c6-a913-4a4a31fd199e",
   "metadata": {},
   "source": [
    "### Identification of Farm Land using Satellite Imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e54339-8533-487a-9b46-61e8dfdb8bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_satellite_image(lat, lng, config):\n",
    "    \"\"\"\n",
    "    Fetches a satellite image from the Sentinel-2 satellite data for a given latitude and longitude.\n",
    "\n",
    "    Parameters:\n",
    "        lat (float): Latitude of the location.\n",
    "        lng (float): Longitude of the location.\n",
    "        config (SHConfig): Configuration object for Sentinel Hub requests.\n",
    "\n",
    "    Returns:\n",
    "        array: A numpy array representing the fetched satellite image, or None if no images are available.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define the bounding box around the specified latitude and longitude\n",
    "        bbox_coords = [lng - 0.009, lat - 0.009, lng + 0.009, lat + 0.009]\n",
    "        resolution = 10  # Spatial resolution in meters\n",
    "        bbox = BBox(bbox=bbox_coords, crs=CRS.WGS84)\n",
    "        size = bbox_to_dimensions(bbox, resolution=resolution)\n",
    "\n",
    "        # Sentinel Hub evalscript to fetch the Red and NIR bands\n",
    "        evalscript_all_bands = \"\"\"\n",
    "            //VERSION=3\n",
    "            function setup() {\n",
    "                return {\n",
    "                    input: [\"B04\", \"B08\"],  // Red and NIR bands\n",
    "                    output: { bands: 2 }\n",
    "                };\n",
    "            }\n",
    "            function evaluatePixel(sample) {\n",
    "                return [sample.B04, sample.B08];\n",
    "            }\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a Sentinel Hub request for the specified bounding box, time interval, and bands\n",
    "        request = SentinelHubRequest(\n",
    "            data_folder='/mnt/data',\n",
    "            evalscript=evalscript_all_bands,\n",
    "            input_data=[\n",
    "                SentinelHubRequest.input_data(\n",
    "                    data_collection=DataCollection.SENTINEL2_L2A,\n",
    "                    time_interval=('2020-06-01', '2020-06-30'),\n",
    "                )\n",
    "            ],\n",
    "            responses=[\n",
    "                SentinelHubRequest.output_response('default', MimeType.TIFF)\n",
    "            ],\n",
    "            bbox=bbox,\n",
    "            size=size,\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "        # Fetch the data and return the first image, if available\n",
    "        images = request.get_data()\n",
    "        return images[0] if images else None\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def calculate_ndvi(image):\n",
    "    \"\"\"\n",
    "    Calculates the NDVI for a given satellite image.\n",
    "\n",
    "    Parameters:\n",
    "        image (array): A numpy array representing the satellite image with Red and NIR bands.\n",
    "\n",
    "    Returns:\n",
    "        float: The average NDVI value for the image.\n",
    "    \"\"\"\n",
    "    # Extract the Red and NIR bands as floats for calculation\n",
    "    red = image[:, :, 0].astype(float)\n",
    "    nir = image[:, :, 1].astype(float)\n",
    "    # Calculate the NDVI, handling division by zero and invalid values\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        ndvi = (nir - red) / (nir + red)\n",
    "    avg_ndvi = np.nanmean(ndvi)\n",
    "    return avg_ndvi\n",
    "\n",
    "def process_dataset(filepath, savepath, config):\n",
    "    \"\"\"\n",
    "    Process addresses in a dataset by fetching satellite images, calculating NDVI, and updating the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        filepath (str): The file path to the input dataset file.\n",
    "        savepath (str): The file path to save the modified dataset file.\n",
    "        config (SHConfig): Configuration object for Sentinel Hub requests.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)  # Read dataset into DataFrame\n",
    "\n",
    "    # Initialize a new column to store the farm evaluation result\n",
    "    df['farmland'] = 'Not Evaluated'\n",
    "\n",
    "    # Iterate over each row in the DataFrame\n",
    "    batch_size = 100\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing\"):\n",
    "        lat, lng = row['latitude'], row['longitude']\n",
    "        image = get_satellite_image(lat, lng, config)\n",
    "        if image is not None:\n",
    "            avg_ndvi = calculate_ndvi(image)\n",
    "            # Consider locations with an NDVI > 0.3 as farms\n",
    "            df.at[index, 'farmland'] = 'Farm' if avg_ndvi > 0.3 else 'Not Farm'\n",
    "        else:\n",
    "            df.at[index, 'farmland'] = 'Image Not Available'\n",
    "\n",
    "        # Save after every batch_size rows\n",
    "        if index % batch_size == 0 and index != 0:\n",
    "            df.to_csv(savepath, index=False)\n",
    "            print(f\"Partial dataset saved at {savepath}\")\n",
    "\n",
    "    # Save the final dataset\n",
    "    df.to_csv(savepath, index=False)  # Save updated DataFrame to CSV\n",
    "    print(\"Final dataset saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration for accessing the Sentinel Hub API\n",
    "    config = SHConfig()\n",
    "    config.sh_client_id = '7aa9c3d4-cf4a-4621-a3c5-a9774a7486fc'\n",
    "    config.sh_client_secret = 'z1rgJaayLd2802nqR6XcEfY951sKrMBE'\n",
    "\n",
    "    # Run the process_dataset function with input and output file paths and configuration\n",
    "    process_dataset('/Users/rakkshetsinghaal/Desktop/Yale University/GLBL 6060/GLBL6060/Final Project - Code and Dataset/address_donor_combined_data.csv', '/Users/rakkshetsinghaal/Desktop/Yale University/GLBL 6060/GLBL6060/Final Project - Code and Dataset/updated_address_donor_combined_data.csv', config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bb3fef-06c8-4637-91f7-1a8015912ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the updated combined dataset\n",
    "df = pd.read_csv('/Users/rakkshetsinghaal/Desktop/Yale University/GLBL 6060/Final Project/updated_address_donor_combined_data.csv')\n",
    "\n",
    "# Create a new column 'agri_land' based on the condition\n",
    "df['agri_land'] = df['farmland'].apply(lambda x: 1 if x == \"Farm\" else 0)\n",
    "\n",
    "# Write the modified DataFrame to a new CSV file\n",
    "df.to_csv('/Users/rakkshetsinghaal/Desktop/Yale University/GLBL 6060/Final Project/complete_combined_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352bc344-9f04-4a85-9262-0fef114c7239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the combined data\n",
    "df_combined = pd.read_csv('/Users/rakkshetsinghaal/Desktop/Yale University/GLBL 6060/GLBL6060/Final Project - Code and Dataset/complete_combined_data.csv')\n",
    "\n",
    "# Define a function to set the value of agri_sector based on the conditions\n",
    "def set_agri_sector(row):\n",
    "    if row['agri_content'] == 1 or row['agri_land'] == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the function to create the agri_sector column\n",
    "df_combined['agri_sector'] = df_combined.apply(lambda row: set_agri_sector(row), axis=1)\n",
    "\n",
    "# Write the updated DataFrame to a new CSV file\n",
    "df_combined.to_csv('/Users/rakkshetsinghaal/Desktop/Yale University/GLBL 6060/GLBL6060/Final Project - Code and Dataset/final_donation_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60586574-284c-4052-be77-31cf199900fb",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c25b3-01ca-443c-84ef-58d35065b73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the specified file path\n",
    "file_path = '/Users/rakkshetsinghaal/Desktop/Yale University/GLBL 6060/GLBL6060/Final Project - Code and Dataset/final_donations_dataset.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Filtering data for the agriculture sector only\n",
    "agri_data = data[data['agri_sector'] == 1]\n",
    "\n",
    "# Grouping data by year and party, and summing donation amounts\n",
    "party_yearly_agri = agri_data.groupby(['year', 'party'])['amount'].sum().unstack(fill_value=0)\n",
    "\n",
    "# Plotting the first graph: Aggregate Donated Amount From Agriculture Sector Party-wise Over the Years\n",
    "plt.figure(figsize=(10, 6))\n",
    "party_yearly_agri.plot(kind='line')\n",
    "plt.title('Aggregate Donated Amount From Agriculture Sector Party-wise Over the Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Total Donation Amount')\n",
    "plt.grid(True)\n",
    "plt.legend(title='Party')\n",
    "plt.savefig('agri_donations_partywise.png')\n",
    "plt.show()\n",
    "\n",
    "# Grouping data by year and agriculture sector status, and summing donation amounts\n",
    "sector_yearly_agri = data.groupby(['year', 'agri_sector'])['amount'].sum().unstack(fill_value=0)\n",
    "\n",
    "# Renaming columns for clarity in the plot\n",
    "sector_yearly_agri.columns = ['Non-Agriculture Sector', 'Agriculture Sector']\n",
    "\n",
    "# Plotting the second graph: Aggregate Donated Amount From Agriculture and Non-Agriculture Sectors Over the Years\n",
    "plt.figure(figsize=(10, 6))\n",
    "sector_yearly_agri.plot(kind='line')\n",
    "plt.title('Aggregate Donated Amount From Agriculture and Non-Agriculture Sectors Over the Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Total Donation Amount')\n",
    "plt.grid(True)\n",
    "plt.legend(title='Sector')\n",
    "plt.savefig('agri_vs_non_agri_donations.png')\n",
    "plt.show()\n",
    "\n",
    "# Grouping data by year, party, and agriculture sector status, and summing donation amounts\n",
    "party_sector_yearly = data.groupby(['year', 'party', 'agri_sector'])['amount'].sum().unstack(fill_value=0)\n",
    "\n",
    "# Renaming columns for clarity in the plot\n",
    "party_sector_yearly.columns = ['Non-Agriculture Sector', 'Agriculture Sector']\n",
    "\n",
    "# Plotting the third graph: Aggregate Donated Amount From Agriculture and Non-Agriculture Sectors Party-wise Over the Years\n",
    "plt.figure(figsize=(12, 8))\n",
    "for party in party_sector_yearly.index.get_level_values(1).unique():\n",
    "    party_data = party_sector_yearly.xs(party, level='party')\n",
    "    plt.plot(party_data.index, party_data['Agriculture Sector'], label=f'{party} - Agriculture')\n",
    "    plt.plot(party_data.index, party_data['Non-Agriculture Sector'], label=f'{party} - Non-Agriculture', linestyle='--')\n",
    "\n",
    "plt.title('Aggregate Donated Amount From Agriculture and Non-Agriculture Sectors Party-wise Over the Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Total Donation Amount')\n",
    "plt.grid(True)\n",
    "plt.legend(title='Party and Sector', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('agri_vs_non_agri_partywise_donations.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb28dcb-f24c-4849-b429-c61871bb5d33",
   "metadata": {},
   "source": [
    "## Mapping Donations Over the Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67a2e63-b19f-4db0-9f2c-1c4239fe7aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "file_path = '/Users/rakkshetsinghaal/Desktop/Yale University/GLBL 6060/GLBL6060/Final Project - Code and Dataset/final_donations_dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Filter out any rows that may have NaN values in 'latitude', 'longitude', or 'year'\n",
    "df = df.dropna(subset=['latitude', 'longitude', 'year'])\n",
    "\n",
    "# Create a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude))\n",
    "gdf.set_crs('EPSG:4326', inplace=True)  # Set the CRS for latitude and longitude\n",
    "\n",
    "# Load the boundary of India\n",
    "india = gpd.read_file(\"https://naturalearth.s3.amazonaws.com/110m_cultural/ne_110m_admin_0_countries.zip\")\n",
    "india = india[india.NAME == \"India\"]\n",
    "\n",
    "# Clip the points to only those within India\n",
    "gdf = gpd.clip(gdf, india)\n",
    "\n",
    "# Initialize the map\n",
    "india_map = folium.Map(location=[20.5937, 78.9629], zoom_start=5)\n",
    "\n",
    "# Create an empty list to store the data for the animated map\n",
    "time_indexed_data = []\n",
    "\n",
    "unique_years = sorted(gdf['year'].unique())\n",
    "\n",
    "for year in unique_years:\n",
    "    data = gdf[gdf['year'] == year]\n",
    "    features = data.apply(\n",
    "        lambda row: {\n",
    "            'type': 'Feature',\n",
    "            'geometry': {\n",
    "                'type': 'Point',\n",
    "                'coordinates': [row['longitude'], row['latitude']]\n",
    "            },\n",
    "            'properties': {\n",
    "                'time': str(year),\n",
    "                'style': {\n",
    "                    'color': 'blue' if row['agri_sector'] == 1 else 'yellow',\n",
    "                    'radius': 10,\n",
    "                },\n",
    "                'icon': 'circle',\n",
    "                'popup': 'Sector: Agriculture' if row['agri_sector'] == 1 else 'Sector: Non-Agriculture',\n",
    "            }\n",
    "        }, axis=1\n",
    "    ).tolist()\n",
    "    time_indexed_data.append({'type': 'FeatureCollection', 'features': features})\n",
    "\n",
    "# Create the timestamped geoJSON\n",
    "plugins.TimestampedGeoJson({\n",
    "    'type': 'FeatureCollection',\n",
    "    'features': sum((x['features'] for x in time_indexed_data), [])\n",
    "}, period='P1Y', add_last_point=True, auto_play=True).add_to(india_map)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "india_map.save('/Users/rakkshetsinghaal/Desktop/Yale University/GLBL 6060/GLBL6060/Final Project - Code and Dataset/india_animated_map.html')\n",
    "india_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3ceeb7-1435-4164-97b0-28d79aee3b0c",
   "metadata": {},
   "source": [
    "## Neural Network Prediction of Agricultural Sector Donations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e757f-d9e0-4864-af5d-451124f14b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable for polynomial features transformer\n",
    "poly_transformer = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "def load_and_preprocess_data(filepath, party):\n",
    "    \"\"\"\n",
    "    Load and preprocess data from a CSV file, adding features for analysis.\n",
    "\n",
    "    Parameters:\n",
    "        filepath (str): The file path to the input dataset file.\n",
    "        party (str): The political party for which the data is being processed ('BJP' or 'INC').\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing features, target, and aggregated historical data.\n",
    "    \"\"\"\n",
    "    global poly_transformer\n",
    "    data = pd.read_csv(filepath)\n",
    "    agri_data = data[data['agri_sector'] == 1]\n",
    "    \n",
    "    # Aggregate donation amounts by year\n",
    "    aggregated_data = agri_data.groupby('year')['amount'].sum().reset_index()\n",
    "    \n",
    "    # Add features\n",
    "    features = aggregated_data[['year']]\n",
    "    features.loc[:, 'policy_year'] = features['year'].apply(lambda x: 1 if x >= 2022 else 0)\n",
    "    features.loc[:, 'party_in_power'] = features['year'].apply(lambda x: 1 if x >= 2014 else 0)\n",
    "    features.loc[:, 'party'] = 1 if party == 'BJP' else 0  # 1 for BJP, 0 for INC\n",
    "    \n",
    "    # Apply polynomial transformation to the 'year' feature\n",
    "    year_poly = poly_transformer.fit_transform(features[['year']])\n",
    "    feature_names = ['year'] + [f'year^{i}' for i in range(2, poly_transformer.degree + 1)]\n",
    "    features_poly = pd.DataFrame(year_poly, columns=feature_names)\n",
    "    features = pd.concat([features, features_poly.drop('year', axis=1)], axis=1)\n",
    "    \n",
    "    # Add interaction term for party_in_power and policy_year to model the impact of policies by the ruling party\n",
    "    features['power_policy_interaction'] = features['party_in_power'] * features['policy_year']\n",
    "    \n",
    "    target = aggregated_data['amount']\n",
    "    return features, target, aggregated_data\n",
    "\n",
    "def build_model(input_shape):\n",
    "    \"\"\"\n",
    "    Build and compile a neural network model for regression.\n",
    "\n",
    "    Parameters:\n",
    "        input_shape (int): The input shape of the model.\n",
    "\n",
    "    Returns:\n",
    "        Sequential: A compiled Keras Sequential model.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_shape,)),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)  # Output layer\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_for_party(filepath, party):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model for a given political party.\n",
    "\n",
    "    Parameters:\n",
    "        filepath (str): The file path to the input dataset file.\n",
    "        party (str): The political party for which the model is being trained ('BJP' or 'INC').\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the trained model, scaler, average loss, and historical data.\n",
    "    \"\"\"\n",
    "    features, target, historical_data = load_and_preprocess_data(filepath, party)\n",
    "    scaler = StandardScaler()\n",
    "    target_normalized = scaler.fit_transform(target.values.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    fold_no = 1\n",
    "    losses = []\n",
    "    \n",
    "    for train, test in kfold.split(features, target_normalized):\n",
    "        X_train, X_test = features.iloc[train], features.iloc[test]\n",
    "        y_train, y_test = target_normalized[train], target_normalized[test]\n",
    "\n",
    "        model = build_model(X_train.shape[1])\n",
    "        model.fit(X_train, y_train, epochs=1000, batch_size=320, validation_data=(X_test, y_test),\n",
    "                  verbose=0, callbacks=[EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)])\n",
    "        \n",
    "        scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "        print(f'Party {party} - Scores for fold {fold_no}: MAE of {scores[1]}, MSE of {scores[2]}')\n",
    "        losses.append(scores[0])\n",
    "        fold_no += 1\n",
    "\n",
    "    return model, scaler, np.mean(losses), historical_data\n",
    "\n",
    "def predict_future_donations_by_party(model, scaler, start_year, policy_year, party_in_power, party):\n",
    "    \"\"\"\n",
    "    Predict future donations for a given political party.\n",
    "\n",
    "    Parameters:\n",
    "        model (Sequential): The trained Keras model for donation prediction.\n",
    "        scaler (StandardScaler): The fitted scaler object for target normalization.\n",
    "        start_year (int): The starting year for predictions.\n",
    "        policy_year (int): Binary indicator for policy implementation year.\n",
    "        party_in_power (int): Binary indicator for party in power.\n",
    "        party (str): The political party for which predictions are being made ('BJP' or 'INC').\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing predicted donation amounts for the next three years.\n",
    "    \"\"\"\n",
    "    global poly_transformer\n",
    "    predictions = {}\n",
    "    for year in range(start_year, start_year + 5):\n",
    "        year_poly = poly_transformer.transform([[year]])\n",
    "        input_features = np.concatenate((\n",
    "            [year, policy_year, party_in_power, 1 if party == 'BJP' else 0],\n",
    "            year_poly[0, 1:],\n",
    "            [party_in_power * policy_year]  # Interaction term\n",
    "        ))\n",
    "        input_features = input_features.reshape(1, -1)\n",
    "        predicted_normalized_amount = model.predict(input_features)\n",
    "        predicted_amount = scaler.inverse_transform(predicted_normalized_amount.reshape(-1, 1))\n",
    "        predictions[year] = predicted_amount[0, 0]\n",
    "    return predictions\n",
    "\n",
    "def visualize_donations(aggregated_data, predictions, party):\n",
    "    \"\"\"\n",
    "    Visualize historical and predicted donation trends for a political party.\n",
    "\n",
    "    Parameters:\n",
    "        aggregated_data (DataFrame): The aggregated historical donation data.\n",
    "        predictions (dict): A dictionary containing predicted donation amounts for the next three years.\n",
    "        party (str): The political party for visualization ('BJP' or 'INC').\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    years = aggregated_data['year']\n",
    "    amounts = aggregated_data['amount']\n",
    "    plt.plot(years, amounts, label=f'Actual Donations ({party})', linestyle='-')\n",
    "    \n",
    "    prediction_years = np.array(list(predictions.keys()))\n",
    "    prediction_amounts = np.array(list(predictions.values()))\n",
    "    if len(prediction_years) > 0 and len(prediction_amounts) > 0:\n",
    "        plt.plot(prediction_years, prediction_amounts, label=f'Predicted Donations ({party})', linestyle='--')\n",
    "    \n",
    "    plt.title(f'Year-wise Donation Trends for {party}')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Aggregated Donation Amount')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'year-wise_donation_trend_{party}.png')\n",
    "    plt.show()\n",
    "\n",
    "parties = ['BJP', 'INC']\n",
    "predictions_by_party = {}\n",
    "party_in_power_status = 1  # Since BJP is in power\n",
    "\n",
    "for party in parties:\n",
    "    model, scaler, average_loss, historical_data = train_and_evaluate_for_party('/Users/rakkshetsinghaal/Desktop/Yale University/GLBL 6060/GLBL6060/Final Project - Code and Dataset/final_donations_dataset.csv', party)\n",
    "    print(f'Party {party} - Average Loss: {average_loss}')\n",
    "    predictions = predict_future_donations_by_party(model, scaler, 2022, 1, party_in_power_status, party)\n",
    "    predictions_by_party[party] = predictions\n",
    "    visualize_donations(historical_data, predictions, party)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
