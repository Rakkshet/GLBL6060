{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bae5e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install and import all the required softwares for this homeowork\n",
    "!pip install pdfplumber\n",
    "!pip install vaderSentiment\n",
    "\n",
    "import requests\n",
    "import nltk\n",
    "import pycountry\n",
    "import pdfplumber\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist, word_tokenize, sent_tokenize, bigrams, trigrams, sentiment\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fd622f-35f3-4c1b-bcda-75858b33916d",
   "metadata": {},
   "source": [
    "1. In class we wrote code to get all of the links from the G77 statements website: https://www.g77.org/statement/index.php\n",
    "Please write code that iterates through each of the links and formats them properly so that they are a readable url. Print them to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d104353-eec8-448a-bb24-fd8ac0917544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL to be scraped\n",
    "url = \"https://www.g77.org/statement/\"\n",
    "\n",
    "# Send an HTTP GET request to the URL and store the response\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all the anchor tags (<a>) in the HTML content\n",
    "links = soup.find_all(\"a\")\n",
    "\n",
    "# Initialize an empty list to store the extracted links\n",
    "list_links = []\n",
    "\n",
    "# Loop through each anchor tag and extract the \"href\" attribute\n",
    "for link in links:\n",
    "    href = link.get(\"href\")    \n",
    "    # Check if the \"href\" attribute contains the specified substring\n",
    "    if \"getstatement.php?id\" in href:\n",
    "        # Combine the base URL and the relative URL to get the full URL\n",
    "        full_url = urljoin(url, href)\n",
    "        # Append the full URL to the list of links\n",
    "        list_links.append(full_url)\n",
    "\n",
    "# Print the final list of extracted links\n",
    "print(list_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7459f03-1ae9-4042-9a2d-b3cf8e419e39",
   "metadata": {},
   "source": [
    "2. Now modify your code so that the urls are saved as a list. Iterate through the list, open each url, and extract the text from each, and save the text to a file called G77_2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043a0964-f071-4ac1-af9d-623f32fd1d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the text content of statements\n",
    "statement_texts = []\n",
    "\n",
    "# Iterate through each URL in the provided list of links\n",
    "for statement_url in list_links:\n",
    "    # Send a GET request to the URL to retrieve the HTML content of the statement page\n",
    "    statement_response = requests.get(statement_url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if statement_response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        statement_soup = BeautifulSoup(statement_response.content, 'html.parser')\n",
    "\n",
    "        # Find all paragraphs in the HTML content\n",
    "        paragraphs = statement_soup.find_all(\"p\")\n",
    "\n",
    "        # Extract text from each paragraph and join them with two newlines as separators\n",
    "        statement_text = '\\n\\n'.join([paragraph.text for paragraph in paragraphs])\n",
    "\n",
    "        # Append the extracted text to the list of statement texts\n",
    "        statement_texts.append(statement_text)\n",
    "    else:\n",
    "        # Print an error message if the request fails, specifying the URL that failed\n",
    "        print(f\"Failed to retrieve statement from {statement_url}\")\n",
    "\n",
    "# Open a text file named \"G77_2024.txt\" in write mode with UTF-8 encoding\n",
    "with open(\"G77_2024.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    # Write the joined statement texts to the file, separated by \"---\"\n",
    "    file.write(\"\\n\\n---\\n\\n\".join(statement_texts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffbe2f5-8be0-4471-909d-6eb16c99df5b",
   "metadata": {},
   "source": [
    "3. Perform a word frequency distribution on the text and visualize the top 20 words used in these statements.( Please remove any stopwords first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6273778c-2dfd-441d-a816-ae9fc617357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary library for stopwords and tokenization\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "# Tokenizing the input text\n",
    "tokens = word_tokenize(statement_text)\n",
    "\n",
    "# Initializing an empty list to store non-stopwords in lowercase\n",
    "new_list = []\n",
    "\n",
    "# Loop through each tokenized word, convert to lowercase, and filter out stopwords\n",
    "for word in tokens:\n",
    "    word = word.lower()\n",
    "    if word not in stopwords:\n",
    "        new_list.append(word)\n",
    "\n",
    "# Creating a frequency distribution object from the filtered list of words\n",
    "fdis = FreqDist(new_list)\n",
    "\n",
    "# Displaying the 20 most common words and their frequencies\n",
    "print(fdis.most_common(20))\n",
    "\n",
    "# Plotting the frequency distribution of the 20 most common words\n",
    "print(fdis.plot(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8757724c-cabc-434a-bfcb-0cadf572c9cb",
   "metadata": {},
   "source": [
    "4. Here is a link to a speech made by President Trump on January 6: https://www.npr.org/2021/02/10/966396848/read-trumps-jan-6-speech-a-key-part-of-impeachment-trial. Not all of the text is his speech. Some of the text is the analysis. Please extract Donald Trumps speech only, remove stopwords and perform a word frequency distribution and visualize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0840e610-5927-4e83-8f14-61ac6090eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the URL to the target webpage containing Trump's Jan 6 speech\n",
    "url = \"https://www.npr.org/2021/02/10/966396848/read-trumps-jan-6-speech-a-key-part-of-impeachment-trial\"\n",
    "\n",
    "# Send a GET request to the URL and store the response\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content of the webpage\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Initialize an empty string to store the extracted speech text\n",
    "speech_text = \"\"\n",
    "\n",
    "# Iterate through all 'p' (paragraph) elements on the webpage and concatenate their text\n",
    "for paragraph in soup.find_all('p'):\n",
    "    speech_text += paragraph.text + \"\\n\"\n",
    "\n",
    "# Tokenize the speech text into individual words\n",
    "tokens = word_tokenize(speech_text)\n",
    "\n",
    "# Create a list of lowercase words, excluding common English stopwords\n",
    "filtered_words = [word.lower() for word in tokens if word.lower() not in stopwords]\n",
    "\n",
    "# Create a frequency distribution of the filtered words\n",
    "fdis = FreqDist(filtered_words)\n",
    "\n",
    "# Print the 20 most common words along with their frequencies\n",
    "print(fdis.most_common(20))\n",
    "\n",
    "# Plot a bar chart of the frequency distribution for the top 20 words\n",
    "print(fdis.plot(20, cumulative=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbf40bd-b008-428d-81cb-33e73a0d79b3",
   "metadata": {},
   "source": [
    "5. Here is a link to a github repo that contains Donald Trump's speeches: https://github.com/ryanmcdermott/trump-speeches/blob/master/speeches.txt\n",
    "\n",
    "What are the 10 most common things Donald Trump \"loves?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacf2c16-db85-476f-a5a4-f06b6c8c71a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of the file \"speeches.txt\" and store it in the variable speeches_text\n",
    "file_path = \"speeches.txt\"\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    speeches_text = file.read()\n",
    "\n",
    "# Tokenize the content of the file into individual words\n",
    "tokens = word_tokenize(speeches_text)\n",
    "\n",
    "# Convert all words to lowercase and filter out stopwords\n",
    "filtered_words = [word.lower() for word in tokens if word.lower() not in stopwords]\n",
    "\n",
    "# Create a list to store words that follow the word \"love\" in the speeches\n",
    "trump_love = []\n",
    "for i in range(len(filtered_words) - 1):\n",
    "    if filtered_words[i] == \"love\":\n",
    "        trump_love.append(filtered_words[i + 1])\n",
    "\n",
    "# Calculate the frequency distribution of words in the filtered list\n",
    "fdis = FreqDist(filtered_words)\n",
    "\n",
    "# Print the 10 most common words and their frequencies\n",
    "print(fdis.most_common(10))\n",
    "\n",
    "# Plot the frequency distribution of the top 10 words\n",
    "print(fdis.plot(10, cumulative=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75761f2",
   "metadata": {},
   "source": [
    "6. Which are the top 5 countries Trump mentions in his speeches, besides America or the United States of America?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8e2897-7aee-4e1d-9830-e19603953c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of lowercase country names using the pycountry library\n",
    "countries = [country.name.lower() for country in pycountry.countries]\n",
    "\n",
    "# Specifying the file path for the input text file\n",
    "file_path = \"speeches.txt\"\n",
    "\n",
    "# Opening and reading the content of the specified text file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    speeches_text = file.read()\n",
    "\n",
    "# Tokenizing the text into words\n",
    "tokens = word_tokenize(speeches_text)\n",
    "\n",
    "# Filtering out words related to \"America\", \"United\", and \"States\" from the tokenized words\n",
    "filtered_usa = [word for word in tokens if word not in [\"america\", \"united\", \"states\"]]\n",
    "\n",
    "# Filtering further to retain only words that match with country names\n",
    "filtered_countries = [word for word in filtered_usa if word in countries]\n",
    "\n",
    "# Calculating the frequency distribution of the filtered country names\n",
    "country_frequencies = FreqDist(filtered_countries)\n",
    "\n",
    "# Extracting the top 5 countries (excluding \"America\" and \"United States\") based on frequency\n",
    "top_countries = country_frequencies.most_common(6)[1:6]\n",
    "\n",
    "# Displaying the top 5 countries and their respective mention counts\n",
    "print(\"Top 5 countries mentioned (excluding America and the United States):\")\n",
    "for country, count in top_countries:\n",
    "    print(f\"{country}: {count} mentions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6a6463-1686-44f0-b155-caa1537090ac",
   "metadata": {},
   "source": [
    "In class we talked about sentence tokenizers that could be useful in determining when sentences might be repeated. One application we discussed was in political speeches, when certain sentences or phrases are repeated to show a candidates coaching, focus or in marketing terms, \"messaging.\" Here is an analyis from https://www.vox.com/2016/8/18/12423688/donald-trump-speech-style-explained-by-linguists of the \"salesman\" techniques that Trump uses:\n",
    "\"Trump’s speeches can be appealing because he uses a lot of salesmen’s tricks. Lakoff, for his part, has an explanation for why Trump’s style of speaking is so appealing to many. Many of Trump’s most famous catchphrases are actually versions of time-tested speech mechanisms that salesmen use. They’re powerful because they help shape our unconscious. Take, for example, Trump’s frequent use of \"Many people are saying...\" or \"Believe me\" — often right after saying something that is baseless or untrue. This tends to sound more trustworthy to listeners than just outright stating the baseless claim, since Trump implies that he has direct experience with what he’s talking about. At a base level, Lakoff argues, people are more inclined to believe something that seems to have been shared. Or when Trump keeps calling Clinton \"crooked,\" or keeps referring to terrorists as \"radical Muslims,\" he’s strengthening the association through repetition. He also calls his supporters \"folks,\" to show he is one of them (though many politicians employ this trick). Trump doesn’t repeat phrases and adjectives because he is stalling for time, Liberman says; for the most part, he’s providing emphasis and strengthening the association.\n",
    "These are normal techniques, particularly in conversational speech. \"Is he reading cognitive science? No. He has 50 years of experience as a salesman who doesn’t care who he is selling to,\" Lakoff says. On this account, Trump uses similar methods in his QVC-style pitch of steaks and vodka as when he talks about his plan to stop ISIS.\"He has been doing this for a very long time as a salesman — that’s what he is best at,\" Lakoff says.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367b87fc-e344-4fdf-846b-4b3b52eeb745",
   "metadata": {},
   "source": [
    "\n",
    "7. Perform a frequency analysis that provides evidence for the assertion made in the Vox article. \n",
    "\n",
    "Consider tokenizing by unigram (one word), bigram(two words), trigram(three words) or more, or whole sentences or multiple approaches that help us understand the most common Trump linguistic characteristcs. Use your evidence and words to describe what you found. This is a fairly open ask. Don't just execute code. Tell me and show me something interesting!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d985318-7969-42b4-802e-3cfbe5b55cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code fetches the content of a specified URL and extracts text from paragraphs on the page.\n",
    "\n",
    "url = \"https://www.vox.com/2016/8/18/12423688/donald-trump-speech-style-explained-by-linguists\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the HTTP request was successful (status code 200).\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page using BeautifulSoup.\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract all paragraphs from the HTML content.\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    \n",
    "    # Join the text of paragraphs into a single string, separated by two newlines.\n",
    "    statement_text = '\\n\\n'.join([paragraph.text for paragraph in paragraphs])\n",
    "else:\n",
    "    # Print an error message if the HTTP request fails.\n",
    "    print(\"Failed to retrieve!\")\n",
    "\n",
    "# Tokenize the text into sentences.\n",
    "sentences = sent_tokenize(statement_text)\n",
    "\n",
    "# Tokenize the text into words.\n",
    "tokens = word_tokenize(statement_text)\n",
    "\n",
    "# Filter out stopwords (common words that do not carry much meaning).\n",
    "filtered_words = [word.lower() for word in tokens if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "# Create token lists for unigrams, bigrams, trigrams, and fourgrams.\n",
    "tokens_list = list(tokens)\n",
    "bigram_tokens = [(str(bigram[0]), str(bigram[1])) for bigram in list(bigrams(tokens_list))]\n",
    "trigram_tokens = [(str(trigram[0]), str(trigram[1]), str(trigram[2])) for trigram in list(trigrams(tokens_list))]\n",
    "fourgram_tokens = [' '.join(tokens_list[i:i + 4]) for i in range(len(tokens_list) - 3)]\n",
    "\n",
    "# Calculate frequency distributions for sentences, filtered words, bigrams, trigrams, and fourgrams.\n",
    "fdis = FreqDist(sentences)\n",
    "fdis1 = FreqDist(filtered_words)\n",
    "fdis2 = FreqDist(bigram_tokens)\n",
    "fdis3 = FreqDist(trigram_tokens)\n",
    "fdis4 = FreqDist(fourgram_tokens)\n",
    "\n",
    "# Print the 10 most common elements for each frequency distribution.\n",
    "print(fdis.most_common(10))\n",
    "print(fdis1.most_common(10))\n",
    "print(fdis2.most_common(10))\n",
    "print(fdis3.most_common(10))\n",
    "print(fdis4.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53df3047-a64e-41d7-9f1d-bcb9f07395a9",
   "metadata": {},
   "source": [
    "8.Sentiment Analysis\n",
    "Sentiment analysis, also known as opinion mining or emotion AI, is a field of natural language processing (NLP) that focuses on identifying and categorizing opinions or sentiments expressed within text data. The primary goal is to determine the writer's or speaker's attitude towards a particular topic, product, service, or overall context. This attitude can range from positive, negative, to neutral, and may also encompass more nuanced emotions like happiness, anger, sadness, quantitative or emotional.\n",
    "\n",
    "\n",
    "The most basic form of sentiment analysis assigns values to words based on a dictionary of words, from neutral to slightly positive or negative, moderately positive or negative, and extremely positive or negative. Vader is a popular package that analyzes sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efe5f21-0b6e-4830-baa6-0e534deacf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this\n",
    "!pip install vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e0ed00-117c-4c03-b17b-ce70f89dedb3",
   "metadata": {},
   "source": [
    "\n",
    "This will compute a compound score by summing the valence scores of each word int he lexicon and then bnormalized between -1 (most negative) and +1 most positive. This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a \"normalize, weighted composite score\" is accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae180ca0-466c-42ef-9c01-b99f3bb4cd73",
   "metadata": {},
   "source": [
    "let's test opur first sentiment using VADER. VADER is great on social media data which can be messy and contain emojis.\n",
    "We will use the polarity_scores( ) method to obtain the polarity indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ae2bf3-715e-4c59-aada-77cb5b56865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I love that movie!\"\n",
    "score = analyzer.polarity_scores(sentence)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab349ee-9595-4bb9-be2f-9b3408c02bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = \"OMG, this job totally SUX!!\"\n",
    "score = analyzer.polarity_scores(sentence2)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2299e0f2-1739-4892-9007-31e9acfaedd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence3 = '😀' #command+control+space brings up emoji'\n",
    "score = analyzer.polarity_scores(sentence3)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25327b5-995f-45ba-b813-9f851758c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence4 = '😀 😀 😀 😀 😀' #command+control+space brings up emoji'\n",
    "score = analyzer.polarity_scores(sentence4)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3f4b4a-90c9-439f-b1fa-6560a94eb0f8",
   "metadata": {},
   "source": [
    "The global financial crisis (GFC) refers to the period of extreme stress in global financial markets and banking systems between mid 2007 and early 2009. During the GFC, a downturn in the US housing market was a catalyst for a financial crisis that spread from the United States to the rest of the world through linkages in the global financial system. Many banks around the world incurred large losses and relied on government support to avoid bankruptcy. Millions of people lost their jobs as the major advanced economies experienced their deepest recessions since the Great Depression in the 1930s.\n",
    "\n",
    "The Federal Reserve conference calls are a good way to track the sentiment that the Fed had toward the crisis. Did they know how bad it was? Did they think they could fix it? Starting in January of 2008, ending in October of 2008 map the sentiment of the Fed reserve calls.\n",
    "\n",
    "Here are your documents:\n",
    "\n",
    "Jan 2008: https://www.federalreserve.gov/monetarypolicy/files/FOMC20080121confcall.pdf\n",
    "\n",
    "Mar 2008: https://www.federalreserve.gov/monetarypolicy/files/FOMC20080310confcall.pdf\n",
    "\n",
    "July 2008: https://www.federalreserve.gov/monetarypolicy/files/FOMC20080724confcall.pdf\n",
    "\n",
    "Sept 2008: https://www.federalreserve.gov/monetarypolicy/files/FOMC20080929confcall.pdf\n",
    "\n",
    "Oct 2008: https://www.federalreserve.gov/monetarypolicy/files/FOMC20081007confcall.pdf\n",
    "\n",
    "\n",
    "You will need to convert these pdf into text to process. \n",
    "1. Perform a word frequency analysis of each of the calls. Don't forget to remove stopwords.\n",
    "2. Find and present any evidence that the Fed understands that what the US and world is going through is unlike anything that has ever before been experienced. What are the key phrases that convey this idea? How do you find them? Can you use a vector based approach or would you use a custom dictionary?\n",
    "3. Extract any evidence that they thought that the situation could be quickly remedied and would not get as bad as it ended up gettting.\n",
    "4. Parse the key participants and plot their sentiment over time. Who is the most positive? Who is the most negative? Who changes the most?\n",
    "5. Add key makers to your visualization that bring in the key events of 2008.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2b71a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define URLs for different Federal Reserve documents related to specific meetings in 2008\n",
    "url1 = \"https://www.federalreserve.gov/monetarypolicy/files/FOMC20080121confcall.pdf\"\n",
    "file1 = \"Jan 2008\"\n",
    "\n",
    "url2 = \"https://www.federalreserve.gov/monetarypolicy/files/FOMC20080310confcall.pdf\"\n",
    "file2 = \"Mar 2008\"\n",
    "\n",
    "url3 = \"https://www.federalreserve.gov/monetarypolicy/files/FOMC20080724confcall.pdf\"\n",
    "file3 = \"July 2008\"\n",
    "\n",
    "url4 = \"https://www.federalreserve.gov/monetarypolicy/files/FOMC20080929confcall.pdf\"\n",
    "file4 = \"Sept 2008\"\n",
    "\n",
    "url5 = \"https://www.federalreserve.gov/monetarypolicy/files/FOMC20081007confcall.pdf\"\n",
    "file5 = \"Oct 2008\"\n",
    "\n",
    "# Define a custom dictionary containing words related to unprecedented and negative situations\n",
    "custom_dictionary = [\"unprecedented\", \"unlike anything before\", \"never experienced\", \"unique situation\", \n",
    "                     \"novel circumstances\", \"crisis\", \"disaster\", \"catastrophe\", \"calamity\", \"meltdown\",\n",
    "                     \"collapse\", \"chaos\", \"turmoil\", \"downturn\", \"devastation\", \"ruin\", \"failure\", \n",
    "                     \"bankruptcy\", \"ruination\", \"fallout\", \"tragedy\", \"debacle\", \"dismay\", \"upheaval\", \n",
    "                     \"desolation\"]\n",
    "\n",
    "# Define a list of words related to the financial crisis of 2008\n",
    "financial_crisis_words =[\"financial crisis\", \"economic downturn\", \"market collapse\", \"bankruptcy\", \n",
    "                         \"subprime mortgage\", \"credit crunch\", \"housing bubble\", \"stock market crash\",\n",
    "                         \"liquidity crisis\", \"global recession\", \"systemic risk\", \"credit default swaps\", \n",
    "                         \"too big\", \"bailout\", \"economic turmoil\",\"financial instability\"]\n",
    "\n",
    "# Define a list of words related to potential solutions or responses to a crisis\n",
    "solutions = [\"stimulus\", \"intervention\", \"monetary policy\", \"interest rates\", \"liquidity\", \"bailout\", \"rescue\",\n",
    "             \"support\", \"injection\", \"easing\", \"recovery\", \"stabilization\", \"credit facilities\", \n",
    "             \"easing\", \"measures\", \"response\", \"aid\", \"assistance\"]\n",
    "\n",
    "# Define key events in 2008 with associated dates and brief descriptions\n",
    "key_events = {\n",
    "    \"2008-03-10\": \"Collapse of Bear Stearns\",\n",
    "    \"2008-07-24\": \"Fannie Mae and Freddie Mac intervention\",\n",
    "    \"2008-09-29\": \"Lehman Brothers Bankruptcy\",\n",
    "    \"2008-10-07\": \"Emergency Economic Stabilization Act\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2108ddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_2008(file, url):\n",
    "    \"\"\"\n",
    "    Downloads a PDF document from the given URL, analyzes its sentiment, and provides relevant information.\n",
    "\n",
    "    Parameters:\n",
    "    - file (str): The name of the PDF document without extension.\n",
    "    - url (str): The URL of the PDF document to be downloaded.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    Prints:\n",
    "    - Top 20 sentences by frequency in the document.\n",
    "    - Sentences related to unprecedented events.\n",
    "    - Sentences containing solution phrases.\n",
    "    - Final compound sentiment score for the document.\n",
    "    - Sentiment scores for participants mentioned in the document.\n",
    "    - Most positive, most negative, and participant with the most significant change in sentiment.\n",
    "    - Plot of sentiment scores of participants over time with key events highlighted.\n",
    "\n",
    "    Note: I had to use ChatGPT for plotting as I'm facing issues with matplotlib. \n",
    "          So, I am not sure about the accuracy of it.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(f\"{file}.pdf\", \"wb\") as pdf_file:\n",
    "            pdf_file.write(response.content)\n",
    "        with pdfplumber.open(f\"{file}.pdf\") as pdf:\n",
    "            statement_text = \"\"\n",
    "            for page_num in range(len(pdf.pages)):\n",
    "                page = pdf.pages[page_num]\n",
    "                statement_text += page.extract_text()\n",
    "        import os\n",
    "        os.remove(f\"{file}.pdf\")\n",
    "        \n",
    "        \n",
    "        tokens = word_tokenize(statement_text)\n",
    "        filtered_words = [word.lower() for word in tokens if word.lower() not in stopwords.words('english')]\n",
    "        fdis = FreqDist(filtered_words)\n",
    "        print(f\"\\nTop 20 Sentences by Frequency in {file}:\")\n",
    "        print(fdis.most_common(20))\n",
    "        \n",
    "        \n",
    "        key_phrases = [sentence for sentence in sentences if any(phrase in sentence.lower() for phrase in custom_dictionary)]\n",
    "        key_phrases += [sentence for sentence in sentences if any(phrase in sentence.lower() for phrase in financial_crisis_words)]\n",
    "        if key_phrases:\n",
    "            print(f\"\\n Sentences related to Unprecedented Events in {file}:\")\n",
    "            for phrase in key_phrases:\n",
    "                print(f\"- {phrase}\")\n",
    "        else:\n",
    "            print(f\"\\n No key phrases related to unprecedented events found in {file}.\")\n",
    "        \n",
    "        \n",
    "        sol_sentences = [sentence for sentence in sentences if any(phrase in sentence.lower() for phrase in solutions)]\n",
    "        if sol_sentences:\n",
    "            print(f\"\\nSentences Containing Solution Phrases in {file}:\")\n",
    "            for sentence in sol_sentences:\n",
    "                print(f\"- {sentence}\")\n",
    "        else:\n",
    "            print(f\"\\nNo sentences containing solution phrases found in {file}.\")\n",
    "        \n",
    "        \n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        document_score = analyzer.polarity_scores(statement_text)\n",
    "        print(f\"\\nFinal Compound Sentiment Score for {file}: {document_score['compound']}\")        \n",
    "        \n",
    "        \n",
    "        participant_sentences = [sentence for sentence in sent_tokenize(statement_text)\n",
    "                                 if sentence.lower().startswith(('mr.', 'ms.', 'mrs.', 'chairman', 'vice chairman',))]\n",
    "        participant_names = []\n",
    "        for participant_sentence in participant_sentences:\n",
    "            name = participant_sentence.split(',')[0].replace(':', '').strip()\n",
    "            participant_names.append(name)\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        participant_scores = []\n",
    "        for participant_sentence in participant_sentences:\n",
    "            scores = analyzer.polarity_scores(participant_sentence)\n",
    "            participant_scores.append(scores['compound'])\n",
    "        print(f\"\\nSentiment Scores for {file} Participants:\")\n",
    "        for i, (name, scores) in enumerate(zip(participant_names, participant_scores)):\n",
    "            print(f\"{name}: {scores}\")\n",
    "        most_positive_index = max(range(len(participant_scores)), key=lambda i: participant_scores[i])\n",
    "        most_negative_index = min(range(len(participant_scores)), key=lambda i: participant_scores[i])\n",
    "        participant_change_index = max(range(1, len(participant_scores)), key=lambda i: participant_scores[i] - participant_scores[i - 1])\n",
    "        print(f\"\\nMost Positive Participant in {file}:\")\n",
    "        print(participant_names[most_positive_index])\n",
    "        print(f\"\\nMost Negative Participant in {file}:\")\n",
    "        print(participant_names[most_negative_index])\n",
    "        print(f\"\\nParticipant with the Most Significant Change in Sentiment in {file}:\")\n",
    "        print(participant_names[participant_change_index])\n",
    "        \n",
    "        \n",
    "        key_event_indices = [participant_names.index(date) for date in key_events.keys()]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(participant_names, participant_scores, marker='o', linestyle='-', color='b', label='Sentiment Scores')\n",
    "        plt.title(f\"Sentiment Scores of Participants over Time ({file})\")\n",
    "        plt.xlabel(\"Participant Names\")\n",
    "        plt.ylabel(\"Compound Sentiment Score\")\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        for index in key_event_indices:\n",
    "            plt.axvline(x=index, color='r', linestyle='--', label='Key Events')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()        \n",
    "\n",
    "        \n",
    "    else:\n",
    "        print(\"Failed to retrieve!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6187f76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the sentiment_2008 function\n",
    "sentiment_2008(file1, url1)\n",
    "sentiment_2008(file2, url2)\n",
    "sentiment_2008(file3, url3)\n",
    "sentiment_2008(file4, url4)\n",
    "sentiment_2008(file5, url5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6f4772-3593-4372-bf0a-ee00a5f2de22",
   "metadata": {},
   "source": [
    "9. Sentiment analysis is not a perfect science, especially when you are using off-the-shelf packages like VADER. Given what you know about the crisis, do you trust Vader's sentiment analysis? Why or why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4648b4e",
   "metadata": {},
   "source": [
    "While VADER serves as a useful tool for approximating the sentiments, it might not be the best tool to understand the true sentiments of individuals tasked with managing the crisis. For instance, the sentence \"A newish wrinkle here in terms of bank markdowns reflects the deterioration of some of the monoline guarantors,\" which received a neutral score from VADER. In reality, when one reads this sentence, there is a discernible negative undertone, which is not at all captured by VADER. This highlights the inherent challenge in relying solely on sentiment analysis tools, as nuances in language and context may lead to misinterpretations. This showcases the importance of human judgment in comprehending sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b12f39-d9c9-42a2-8cab-351209656845",
   "metadata": {},
   "source": [
    "\n",
    "10. Can you reasonably determine whether a low or negative compound score indicates a negative sentiment from the Fed in 2008? Can you do so with data only from 2008?\n",
    "\n",
    "Provide a reasonable comparison from 2008 values by comparing it to some other timeframe from the FED confernce call historic database. https://www.federalreserve.gov/monetarypolicy/fomc_historical_year.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfb0c38-720a-4b68-917e-1a3849d5b833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define URLs for different Federal Reserve documents related to specific meetings in 2013\n",
    "url6 = \"https://www.federalreserve.gov/monetarypolicy/files/FOMC20130130meeting.pdf\"\n",
    "file6 = \"Jan 2013\"\n",
    "\n",
    "url7 = \"https://www.federalreserve.gov/monetarypolicy/files/FOMC20130320meeting.pdf\"\n",
    "file7 = \"Mar 2013\"\n",
    "\n",
    "url8 = \"https://www.federalreserve.gov/monetarypolicy/files/FOMC20130731meeting.pdf\"\n",
    "file8 = \"July 2013\"\n",
    "\n",
    "url9 = \"https://www.federalreserve.gov/monetarypolicy/files/FOMC20130918meeting.pdf\"\n",
    "file9 = \"Sept 2013\"\n",
    "\n",
    "url10 = \"https://www.federalreserve.gov/monetarypolicy/files/FOMC20131030meeting.pdf\"\n",
    "file10 = \"Oct 2013\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b185dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_2013(file, url):\n",
    "    \"\"\"\n",
    "    Downloads a PDF file from the given URL, analyzes its sentiment,\n",
    "    and provides sentiment scores for overall document and participants.\n",
    "\n",
    "    Parameters:\n",
    "    - file (str): The base name for the PDF file to be saved and analyzed.\n",
    "    - url (str): The URL from which the PDF file will be retrieved.\n",
    "\n",
    "    Prints:\n",
    "    - Final Compound Sentiment Score for the document.\n",
    "    - Sentiment Scores for participants.\n",
    "    - Most Positive, Most Negative, and Participant with the Most Significant Change in Sentiment.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(f\"{file}.pdf\", \"wb\") as pdf_file:\n",
    "            pdf_file.write(response.content)\n",
    "        with pdfplumber.open(f\"{file}.pdf\") as pdf:\n",
    "            statement_text = \"\"\n",
    "            for page_num in range(len(pdf.pages)):\n",
    "                page = pdf.pages[page_num]\n",
    "                statement_text += page.extract_text()\n",
    "        import os\n",
    "        os.remove(f\"{file}.pdf\")\n",
    "\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        document_score = analyzer.polarity_scores(statement_text)\n",
    "        print(f\"\\nFinal Compound Sentiment Score for {file}: {document_score['compound']}\")\n",
    "\n",
    "        participant_sentences = [sentence for sentence in sent_tokenize(statement_text)\n",
    "                                 if sentence.lower().startswith(('mr.', 'ms.', 'mrs.', 'chairman', 'vice chairman',))]\n",
    "        participant_names = []\n",
    "        for participant_sentence in participant_sentences:\n",
    "            name = participant_sentence.split(',')[0].replace(':', '').strip()\n",
    "            participant_names.append(name)\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        participant_scores = []\n",
    "        for participant_sentence in participant_sentences:\n",
    "            scores = analyzer.polarity_scores(participant_sentence)\n",
    "            participant_scores.append(scores['compound'])\n",
    "        print(f\"\\nSentiment Scores for {file} Participants:\")\n",
    "        for i, (name, scores) in enumerate(zip(participant_names, participant_scores)):\n",
    "            print(f\"{name}: {scores}\")\n",
    "        most_positive_index = max(range(len(participant_scores)), key=lambda i: participant_scores[i])\n",
    "        most_negative_index = min(range(len(participant_scores)), key=lambda i: participant_scores[i])\n",
    "        participant_change_index = max(range(1, len(participant_scores)), key=lambda i: participant_scores[i] - participant_scores[i - 1])\n",
    "        print(f\"\\nMost Positive Participant in {file}:\")\n",
    "        print(participant_names[most_positive_index])\n",
    "        print(f\"\\nMost Negative Participant in {file}:\")\n",
    "        print(participant_names[most_negative_index])\n",
    "        print(f\"\\nParticipant with the Most Significant Change in Sentiment in {file}:\")\n",
    "        print(participant_names[participant_change_index])\n",
    "    \n",
    "        \n",
    "    else:\n",
    "        print(\"Failed to retrieve!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4f58aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the sentiment_2013 function\n",
    "sentiment_2013(file6, url6)\n",
    "sentiment_2013(file7, url7)\n",
    "sentiment_2013(file8, url8)\n",
    "sentiment_2013(file9, url9)\n",
    "sentiment_2013(file10, url10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5529362b",
   "metadata": {},
   "source": [
    "By comparing sentiment from 2013 and 2008, it becomes apparent that the compound scores in 2013 exhibit a substantial increase. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
